{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1OkqDKjqHSBeGAzK6m3QZ-aR_4eW6uSkj",
      "authorship_tag": "ABX9TyOZd8Qqto6pxWcZi3VVUsK1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kevin6449/2026_speech/blob/main/gen_translate_cn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 了解 Gemini API 的文件处理功能"
      ],
      "metadata": {
        "id": "4c3TFMFHfRDS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gemini API 可处理传入的 ipynb 文件，并进行推论。时间 上传 ipynb 档案后，Gemini API 就能：\n",
        "\n",
        "说明或回答内容相关问题\n",
        "提供内容的摘要\n",
        "从内容推断\n"
      ],
      "metadata": {
        "id": "CyHbqOF0feva"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c_TGApoN-h9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai"
      ],
      "metadata": {
        "id": "R97GLs0J-8eV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "API_KEY=userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "TKVTDrPt--uG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#genai.configure(api_key=\"YOUR_API_KEY\")\n",
        "\n",
        "# Configure the client library by providing your API key.\n",
        "genai.configure(api_key=API_KEY)"
      ],
      "metadata": {
        "id": "Bl_mLa23_EYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "t-xEdKYzQ8O_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "file_path = '/content/drive/MyDrive/xxxxx..ipynb' # @param {type: \"string\", placeholder: \"[file_path]\", isTemplate: true}\n",
        "\n",
        "# Get the filename from the path\n",
        "filename = os.path.basename(file_path)\n",
        "print(filename) #output: my_file.txt\n",
        "\n",
        "#get the directory from the path.\n",
        "directory = os.path.dirname(file_path)\n",
        "print(directory) #output: /content/drive/MyDrive/data"
      ],
      "metadata": {
        "id": "EKVhTl4RIJiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(file_path, 'r') as f:\n",
        "    content = f.read()\n",
        "    print(content)\n"
      ],
      "metadata": {
        "id": "ZNKriSpxIh6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 使用上传的文件，提示 Gemini API\n",
        "上传档案后，您可以提出参照的 GenerateContent 要求 File API URI选取生成式模型，并以文字提示提供模型 以及上传的文件：\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X1OIdC7wgHEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a Gemini model.\n",
        "model = genai.GenerativeModel(model_name=\"gemini-2.5-flash\")\n",
        "\n",
        "# @title 设定翻译模式\n",
        "translation_mode = \"簡體中文\" # @param [\"簡體中文\", \"繁體中文\", \"英文\"]\n",
        "\n",
        "# 根据选单决定 Prompt\n",
        "if translation_mode == \"簡體中文\":\n",
        "    translate_prompt = \"你可以帮我把 json格式内 source内容 翻译成简體中文吗，保持原来的格式\"\n",
        "elif translation_mode == \"繁體中文\":\n",
        "    translate_prompt = \"你可以幫我把 json格式內 source內容 翻譯成繁體中文嗎，保持原來的格式\"\n",
        "else:\n",
        "    translate_prompt = \"Please translate the source content in json format to English, keeping the original format\"\n",
        "\n",
        "print(f\"目前的 Prompt: {translate_prompt}\")\n",
        "\n",
        "# Prompt the model with text and the previously uploaded image.\n",
        "response = model.generate_content([content, translate_prompt])\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "XKjpMbvG_jtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json =response.text.replace(\"```json\", \"\")\n",
        "json =json.replace(\"```\", \"\")\n",
        "print(json)"
      ],
      "metadata": {
        "id": "DfEuDB5Pxui4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate_file_path = '/content/drive/MyDrive/xxxxx_cn.ipynb' # @param {type: \"string\", placeholder: \"[trans_file_path]\", isTemplate: true}\n",
        "with open(translate_file_path, 'w') as f:\n",
        "    f.write(json)\n"
      ],
      "metadata": {
        "id": "Inan5PK5LKmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6rcnoNMhiXaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jCKEOHK2u7Qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LHQ9g8Ayhht5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
